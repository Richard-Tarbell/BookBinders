---
title: "Bookbinders Case Study"
author: "Austin Vanderlyn, Christine Kelly, Richard Tarbell"
date: "2/16/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(logistf)
library(tidyverse)
library(car)
library(broom)
library(DescTools)
library(ROCR)
library(ResourceSelection)
library(e1071)
```


```{r, include=FALSE}
BBBC_Train <- read_excel("Data/BBBC-Train.xlsx",col_names = TRUE)
BBBC_Test <- read_excel("Data/BBBC-Test.xlsx",col_names = TRUE)
```


### I. Executive Summary - Richard
LM

Logit

SVM

### II. The Problem - Austin

### III. Review of Related Literature - Christine

Linear Regression is a parametric model used to find if any explanatory variables influence the response variable.  There are four assumptions for linear regression.  The first is the linear relationship between X and Y.  The second is normal distribution.  The third is homoscedasticity.  Finally, all observations must be independent of each other.  The model finds the best line to predict the behavior of Y, when X is increased by 1 unit.  Y must be continuous for the model to work.

Logistic Regression is used to predict the probability of an outcome based on given variables, or to see how variables are related to the outcome.  For the logistic model Y is binary and not continuous.  Interpretation of the logistic model is not as straightforward and requires an understanding of odds and odds ratio.  

Support Vector Machine (SVM) is a popular model that was developed in the 1990s by Vladimir Vapnik and Corinna Cortes.  This model finds a hyperplane that separates the data as well as possible and allows some misclassification. To accommodate a non-linear boundary between classes, SVM enlarges the feature space using polynomial terms.  The SVM enlarges this feature space, using kernel tricks, in a way that is efficient with these computations. 

All three of these models are beneficial in providing insights and predictions when applied to marketing campaign selections.

### IV. Methodologies

The first model attempted was a linear regression.  This model is not useful for this dataset because the response variable in linear regression must be continuous.  In this case  the response variable Choice is categorical and must be converted to a factor.  If the regression model is used leaving the Choice as numeric, the information it provides is not useful for the criteria we are trying to meet.  

### V. Data / Cleaning - Richard

The datasets required minimal cleaning. Both the Train (`BBBC_Train`) and Test (`BBBC_Test`) datasets originally had 12 variables and the Training set consisted of 1600 observations while the Testing set had 2300. Performing basic analysis of the data we saw there were no missing values and there happened to be a column in each titled `Observation`. We removed this column because it would add no significant value to our models.

Checking for correlations within the pairwise plot the strongest correlation came out to be between `Last_Purchase` and `First_Purchase` which resemble the months since the customers last purchase and the months since their first purchase. With this being the only positive correlation > 0.7 we may run into the issue of multicollinearity. 

The variables `Choice` and `Gender` were both treated as numeric variables within the data however we converted these to factor variables given their binary values. Further exploring these variables we discovered that approximately 70% of customers who did not a purchase were Males and \~54% of the customers who did make a purchase were also Males. Comparing the customers who did not make a purchase to those who did we find a class imbalance with only 25% of the `Choice` class to being customers who did make a purchase. This may cause issues in model accuracy given that SVMs do not perform well on imbalanced datasets.





### VI. Findings 

### VII. Conclusion and Recommendations



### Appendix


##### Data importing and cleaning


Exploration

Any missing values?
```{r}
anyNA(BBBC_Train)
anyNA(BBBC_Test)
```


Check the size of the training and test datasets
```{r}
dim(BBBC_Train)

dim(BBBC_Test)
```

Class imbalance
```{r}
table(BBBC_Train$Choice)
```

View details of the data
```{r}
str(BBBC_Train)
```



Remove `Observation` variable and convert `Choice` to factor

```{r}
BBBC_Train = subset(BBBC_Train, select = -Observation)
BBBC_Test = subset(BBBC_Test, select = -Observation)

BBBC_Train$Choice = as.factor(BBBC_Train$Choice)
BBBC_Test$Choice = as.factor(BBBC_Test$Choice)
```



```{r, include=FALSE}
# code to get histograms in pair plot
panel.hist <- function(x, ...) {
    usr <- par("usr")
    on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5))
    his <- hist(x, plot = FALSE)
    breaks <- his$breaks
    nB <- length(breaks)
    y <- his$counts
    y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = rgb(0, 1, 1, alpha = 0.5), ...)
    # lines(density(x), col = 2, lwd = 2) # Uncomment to add density lines
}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor)
}
```

```{r, fig.height=8, fig.width=12}
pairs.test = subset(BBBC_Train, select = -Choice)
pairs.test = subset(pairs.test, select = -Gender)

pairs(pairs.test,
      upper.panel = panel.cor,
      diag.panel = panel.hist)
```



```{r}
ggplot(data = BBBC_Train,
       aes(x = factor(ifelse(Choice == 1, "Purchase", "No Purchase")), 
           fill = factor(ifelse(Gender == 0, "Female", "Male")))) + 
    geom_bar(alpha =1, color = "black", stat = "count") + 
    scale_fill_manual(values = c("lavender", "orange")) +
    geom_text(aes(label = scales::percent(..count.. / tapply(..count.., ..x.., sum)[as.character(..x..)])), stat = "count", position = position_stack(vjust=0.5)) +
    labs(fill = "Gender", y="", x="")
```






### Linear Regression Model


```{r}
lm.book <- lm(as.numeric(Choice) ~ ., data = BBBC_Train)
summary(lm.book)
```










### Logistic Regression Model


Change gender to factor for logistic regression model
```{r}
BBBC_Train_Logit = BBBC_Train
BBBC_Test_Logit = BBBC_Test

BBBC_Train_Logit$Gender = as.factor(BBBC_Train_Logit$Gender)

BBBC_Test_Logit$Gender = as.factor(BBBC_Test_Logit$Gender)
```



Create initial logistic regression model
```{r}
glm.train = glm(Choice ~ ., data = BBBC_Train_Logit, family = "binomial")
summary(glm.train)
```

Remove variables with high multicollinearity
```{r}
vif(glm.train)

```

```{r}
glm.train = glm(Choice ~ .-Last_purchase, data = BBBC_Train_Logit, family = "binomial")

vif(glm.train)
```

```{r}
glm.train = glm(Choice ~ .-Last_purchase-First_purchase, data = BBBC_Train_Logit, family = "binomial")

vif(glm.train)
```

Build stepwise model
```{r}
glm.null = glm(Choice ~ 1, data = BBBC_Train_Logit, family = "binomial")
glm.full = glm(Choice ~ .-Last_purchase-First_purchase, data = BBBC_Train_Logit, family = "binomial")

glm.step1 = step(glm.null, scope = list(upper = glm.full), direction = "both", test = "Chisq", trace = FALSE)

summary(glm.step1)
```

Goodness of fit
```{r}
hoslem.test(glm.step1$y, fitted(glm.step1), g = 10)
```

Plot
```{r}
plot(glm.step1, which = 4)
```





```{r}
BBBC_Train_Logit$PredProb = predict.glm(glm.step1, BBBC_Train_Logit, type = "response")
BBBC_Train_Logit$PredChoice = ifelse(BBBC_Train_Logit$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(BBBC_Train_Logit$Choice), as.factor(BBBC_Train_Logit$PredChoice))
```

The accuracy of this model is not the best, but the positive predictive value, which is what we care about, is pretty good.



Run model with test data;
```{r}
BBBC_Test_Logit$PredProb = predict.glm(glm.step1, BBBC_Test_Logit, type = "response")
BBBC_Test_Logit$PredChoice = ifelse(BBBC_Test_Logit$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(BBBC_Test_Logit$Choice), as.factor(BBBC_Test_Logit$PredChoice))
```

The model has better accuracy with the test data, and still has a high positive predictive value. It could get better by calibrating the sensitivity and specificity.


Calculate and plot AUC
```{r}
pred_test = predict(glm.step1, BBBC_Test_Logit, type = "response")
response_test = BBBC_Test_Logit$Choice
predict_test = prediction(pred_test, response_test)
auc_test = round(as.numeric(performance(predict_test, measure = "auc")@y.values),3)
perform = performance(predict_test, "tpr","fpr")
plot(perform, colorize = T, main = "ROC Curve")
text(0.5,0.5, paste("AUC:", auc_test))
```



```{r}
plot(unlist(performance(predict_test, "sens")@x.values),
     unlist(performance(predict_test, "sens")@y.values),
     type = "l",
     lwd = 2,
     ylab = "Sensitivity",
     xlab = "Cutoff",
     main = paste("Maximized Cutoff", "AUC: ", auc_test))

par(new = TRUE)

plot(unlist(performance(predict_test, "spec")@x.values),
     unlist(performance(predict_test, "spec")@y.values),
     type = "l",
     lwd = 2,
     col = "red")

axis(4, at=seq(0, 1, 0.2))
mtext("Specificity", side = 4, col = 'red')

min.diff.glm = which.min(abs(unlist(performance(predict_test, "sens")@y.values) - 
                               unlist(performance(predict_test, "spec")@y.values)))
min.x.glm = unlist(performance(predict_test, "sens")@x.values)[min.diff.glm]
min.y.glm = unlist(performance(predict_test, "sens")@y.values)[min.diff.glm]
optimal.glm = min.x.glm

abline(h = min.y.glm, lty = 3)
abline(v = min.x.glm, lty = 3)
text(min.x.glm,0,paste("optimal threshold=",round(optimal.glm,2)), pos = 4)
```
So now we know that the optimal cutoff threshold is 0.23, so we can refit the predictions to optimize the sensitivity and specificity.

Refit predictions and confusion matrix;
```{r}
BBBC_Test_Logit$PredProb = predict.glm(glm.step1, BBBC_Test_Logit, type = "response")
BBBC_Test_Logit$PredChoice = ifelse(BBBC_Test_Logit$PredProb >= 0.23, 1, 0)
caret::confusionMatrix(as.factor(BBBC_Test_Logit$Choice), as.factor(BBBC_Test_Logit$PredChoice))
```

This doesn't actually look that good. The model's accuracy has dropped, as has the specificity and positive predictive value. Based on these numbers, it would most likely be better to go with the model that has a cutoff of 0.5. 


The best Logit model therefore, is;

log(1/(1-p)) = -0.289 + 1.244*P_Art - 0.088*Frequency - 0.812*Gender1 - 0.294*P_Cook - 0.282*P_DIY + 0.002*Amount_Purchased - 0.196*P_Child

The most influential covariates then, are P_Art(number of art books purchased), Frequency(total number of purchases), Gender, P_Cook(number of cookbooks purchased), P_DIY (number of DIY books purchased), Amount_Purchased (total money spent), and P_Child.

Breaking down how each covariate influences the model;

The odds of a customer buying The Art History of Florence change by a factor of 3.46 with each additional art book purchased, assuming other variables remain constant.

The odds of a customer buying The Art History of Florence change by a factor of 0.915 with each additional book purchased, assuming other variables remain constant.

The odds of a male customer are .443 times that of a female customer, assuming other variables remain constant.

The odds of a customer buying The Art History of Florence change by a factor of 0.745 with each additional cook book purchased, assuming other variables remain constant.

The odds of a customer buying The Art History of Florence change by a factor of 1.002 with each additional dollar spent, assuming other variables remain constant.

The odds of a customer buying The Art History of Florence change by a factor of 1.21 with each additional children's book purchased, assuming other variables remain constant.
```{r}
exp(1.244)
exp(-0.088)
exp(-0.812)
exp(-0.294)
exp(0.002)
exp(0.196)
```

### Midwest Mailing Campaign

Data for the proposed mailing campaign;
50,000 customers
cost of mailing = $0.65 / addressee
cost of book = $15
Selling price of book = $31.95
overhead = 0.45*bookcost










### SVM Model

```{r}
BBBC_Train_SVM = BBBC_Train
BBBC_Test_SVM = BBBC_Test
```

```{r}
str(BBBC_Train_SVM)
```



#### Use training split on BBBC_Train

```{r}
# Splitting data into training and testing sets 70/30 Split
set.seed(1)
tr_ind = sample(nrow(BBBC_Train_SVM), 0.7*nrow(BBBC_Train_SVM), replace=FALSE)
book.train.split = BBBC_Train_SVM[tr_ind,]
book.test.split = BBBC_Train_SVM[-tr_ind,]
```


```{r}
svm_form = Choice ~ .

tuned = tune.svm(svm_form, data = book.train.split, 
                 gamma = seq(.01, .1, by = .01), 
                 cost = seq(.1, 1, by = .1))
```


```{r}
mysvm = svm(formula = svm_form, 
            data = book.train.split, 
            gamma =tuned$best.parameters$gamma, 
            cost = tuned$best.parameters$cost)

summary(mysvm)

```

```{r}
# Predict on the test split
svmpredict = predict(mysvm, 
                     book.test.split, 
                     type = "response")

caret::confusionMatrix(svmpredict, book.test.split$Choice)

```

```{r}
svmpredict = predict(mysvm, 
                     BBBC_Test_SVM, 
                     type = "response")

caret::confusionMatrix(svmpredict, BBBC_Test_SVM$Choice)

```

