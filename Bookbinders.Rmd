---
title: "Bookbinders Case Study"
author: "Austin Vanderlyn, Christine Kelly, Richard Tarbell"
date: "2/16/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(logistf)
library(tidyverse)
library(car)
library(broom)
library(DescTools)
library(ROCR)
library(ResourceSelection)
library(e1071)
```


```{r, include=FALSE}
BBBC_Train <- read_excel("Data/BBBC-Train.xlsx",col_names = TRUE)
BBBC_Test <- read_excel("Data/BBBC-Test.xlsx",col_names = TRUE)
```


### I. Executive Summary - Richard
LM

Logit

SVM

### II. The Problem - Austin

### III. Review of Related Literature - Christine

Linear Regression is a parametric model used to find if any explanatory variables influence the response variable.  There are four assumptions for linear regression.  The first is the linear relationship between X and Y.  The second is normal distribution.  The third is homoscedasticity.  Finally, all observations must be independent of each other.  The model finds the best line to predict the behavior of Y, when X is increased by 1 unit.  Y must be continuous for the model to work.

Logistic Regression is used to predict the probability of an outcome based on given variables, or to see how variables are related to the outcome.  For the logistic model Y is binary and not continuous.  Interpretation of the logistic model is not as straightforward and requires an understanding of odds and odds ratio.  

Support Vector Machine (SVM) is a popular model that was developed in the 1990s by Vladimir Vapnik and Corinna Cortes.  This model finds a hyperplane that separates the data as well as possible and allows some misclassification. To accommodate a non-linear boundary between classes, SVM enlarges the feature space using polynomial terms.  The SVM enlarges this feature space, using kernel tricks, in a way that is efficient with these computations. 

All three of these models are beneficial in providing insights and predictions when applied to marketing campaign selections.

### IV. Methodologies

### V. Data / Cleaning - Richard

### VI. Findings 

### VII. Conclusion and Recommendations



### Appendix


##### Data importing and cleaning


Exploration

```{r}
anyNA(BBBC_Train)
anyNA(BBBC_Test)

```


```{r}
str(BBBC_Train)
head(BBBC_Train)
```

Remove observation variable, not relevant
```{r}
BBBC_Train = subset(BBBC_Train, select = -Observation)
BBBC_Test = subset(BBBC_Test, select = -Observation)
```

Change choice and gender to factors for logistic regression model
```{r}
BBBC_Train_Logit = BBBC_Train
BBBC_Test_Logit = BBBC_Test

BBBC_Train_Logit$Choice = as.factor(BBBC_Train_Logit$Choice)
BBBC_Train_Logit$Gender = as.factor(BBBC_Train_Logit$Gender)
BBBC_Test_Logit$Choice = as.factor(BBBC_Test_Logit$Choice)
BBBC_Test_Logit$Gender = as.factor(BBBC_Test_Logit$Gender)
```




### Linear Regression Model


```{r}
lm.book <- lm(Choice ~ ., data = BBBC_Train)
summary(lm.book)
```










### Logistic Regression Model


Create initial logistic regression model
```{r}
glm.train = glm(Choice ~ ., data = BBBC_Train_Logit, family = "binomial")
summary(glm.train)
```

Remove variables with high multicollinearity
```{r}
vif(glm.train)

```

```{r}
glm.train = glm(Choice ~ .-Last_purchase, data = BBBC_Train_Logit, family = "binomial")

vif(glm.train)
```

```{r}
glm.train = glm(Choice ~ .-Last_purchase-First_purchase, data = BBBC_Train_Logit, family = "binomial")

vif(glm.train)
```

Build stepwise model
```{r}
glm.null = glm(Choice ~ 1, data = BBBC_Train_Logit, family = "binomial")
glm.full = glm(Choice ~ .-Last_purchase-First_purchase, data = BBBC_Train_Logit, family = "binomial")

glm.step1 = step(glm.null, scope = list(upper = glm.full), direction = "both", test = "Chisq", trace = FALSE)

summary(glm.step1)
```

Goodness of fit
```{r}
hoslem.test(glm.step1$y, fitted(glm.step1), g = 10)
```

Plot
```{r}
plot(glm.step1, which = 4)
```


```{r}
BBBC_Train_Logit$PredProb = predict.glm(glm.step1, newdata = BBBC_Train_Logit, type = "response")
BBBC_Train_Logit$PredChoice = ifelse(BBBC_Train_Logit$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(BBBC_Train_Logit$Choice), as.factor(BBBC_Train_Logit$PredChoice))
```










### SVM Model

```{r}
BBBC_Train_SVM = BBBC_Train
BBBC_Test_SVM = BBBC_Test
```

```{r}
# Convert to factors
BBBC_Train_SVM$Choice = as.factor(BBBC_Train_SVM$Choice)
BBBC_Train_SVM$Gender = as.factor(BBBC_Train_SVM$Gender)

BBBC_Test_SVM$Choice = as.factor(BBBC_Test_SVM$Choice)
BBBC_Test_SVM$Gender = as.factor(BBBC_Test_SVM$Gender)
```

#### Use training split on BBBC_Train

```{r}
# Splitting data into training and testing sets 70/30 Split
set.seed(1)
tr_ind = sample(nrow(BBBC_Train_SVM), 0.7*nrow(BBBC_Train_SVM), replace=FALSE)
book.train.split = BBBC_Train_SVM[tr_ind,]
book.test.split = BBBC_Train_SVM[-tr_ind,]
```

```{r}
svm_form = Choice ~ .

tuned = tune.svm(svm_form, data = book.train.split, 
                 gamma = seq(.01, .1, by = .01), 
                 cost = seq(.1, 1, by = .1))
```


```{r}
mysvm = svm(formula = svm_form, 
            data = book.train.split, 
            gamma =tuned$best.parameters$gamma, 
            cost = tuned$best.parameters$cost)

summary(mysvm)

```

```{r}
# Predict on the test split
svmpredict = predict(mysvm, 
                     book.test.split, 
                     type = "response")

caret::confusionMatrix(svmpredict, book.test.split$Choice)

```

```{r}
svmpredict = predict(mysvm, 
                     BBBC_Test_SVM, 
                     type = "response")

caret::confusionMatrix(svmpredict, BBBC_Test_SVM$Choice)

```

