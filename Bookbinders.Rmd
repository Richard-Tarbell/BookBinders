---
title: "Bookbinders Case Study"
author: "Austin Vanderlyn, Christine Kelly, Richard Tarbell"
date: "2/16/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(logistf)
library(tidyverse)
library(car)
library(broom)
library(DescTools)
library(ROCR)
<<<<<<< HEAD
library(ResourceSelection)
```

```{r, include=FALSE}
BBBC_Train <- read_excel("Data/BBBC-Train.xlsx")
BBBC_Test = read_excel("Data/BBBC-Test.xlsx")
=======
library(e1071)
```

```{r, include=FALSE}
BBBC_Train <- read_excel("Data/BBBC-Train.xlsx",col_names = TRUE)
>>>>>>> 478c65954d38802cfa506b8174727f63c10a8778
```


### I. Executive Summary - Richard  

### II. The Problem - Austin

### III. Review of Related Literature - Christine

### IV. Methodologies

### V. Data / Cleaning - Richard

### VI. Findings 

### VII. Conclusion and Recommendations

### Appendix







### Logistic Regression Model

Exploration
```{r}
str(BBBC_Train)
head(BBBC_Train)
```


Remove observation variable, not relevant
```{r}
BBBC_Train_Logit = subset(BBBC_Train, select = -Observation)
BBBC_Test_Logit = subset(BBBC_Test, select = -Observation)
```



Change choice and gender to factors
```{r}
BBBC_Train_Logit$Choice = as.factor(BBBC_Train_Logit$Choice)
BBBC_Train_Logit$Gender = as.factor(BBBC_Train_Logit$Gender)
BBBC_Test_Logit$Choice = as.factor(BBBC_Test_Logit$Choice)
BBBC_Test_Logit$Gender = as.factor(BBBC_Test_Logit$Gender)
```


Create intial logistic regression model
```{r}
glm.train = glm(Choice ~ ., data = BBBC_Train_Logit, family = "binomial")
summary(glm.train)
```


Remove variables with high multicollinearity
```{r}
vif(glm.train)
glm.train = glm(Choice ~ .-Last_purchase, data = BBBC_Train_Logit, family = "binomial")
vif(glm.train)
glm.train = glm(Choice ~ .-Last_purchase-First_purchase, data = BBBC_Train_Logit, family = "binomial")
vif(glm.train)
```


<<<<<<< HEAD
Build stepwise model
```{r}
glm.null = glm(Choice ~ 1, data = BBBC_Train_Logit, family = "binomial")
glm.full = glm(Choice ~ .-Last_purchase-First_purchase, data = BBBC_Train_Logit, family = "binomial")
glm.step1 = step(glm.null, scope = list(upper = glm.full), direction = "both", test = "Chisq", trace = FALSE)
summary(glm.step1)
```


Goodness of fit
```{r}
hoslem.test(glm.step1$y, fitted(glm.step1), g = 10)
```


Plot
```{r}
plot(glm.step1, which = 4)
```





```{r}
BBBC_Train_Logit$PredProb = predict.glm(glm.step1, newdata = BBBC_Train_Logit, type = "response")
BBBC_Train_Logit$PredChoice = ifelse(BBBC_Train_Logit$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(BBBC_Train_Logit$Choice), as.factor(BBBC_Train_Logit$PredChoice))
```

```
=======
```{r}
# Remove the observed column
BBBC_Train <- BBBC_Train[2:12]

head(BBBC_Train)
```


```{r}
anyNA(BBBC_Train)
```


```{r}
str(BBBC_Train)
```

```{r}
# Convert to factors
BBBC_Train$Choice = as.factor(BBBC_Train$Choice)
BBBC_Train$Gender = as.factor(BBBC_Train$Gender)
```


```{r}
# Splitting data into training and testing sets 70/30 Split
set.seed(1)
tr_ind = sample(nrow(BBBC_Train), 0.7*nrow(BBBC_Train), replace=FALSE)
bbbctrain = BBBC_Train[tr_ind,]
bbbctest = BBBC_Train[-tr_ind,]
```

```{r}
svm_form = Choice ~ .

tuned = tune.svm(svm_form, data = bbbctrain, 
                 gamma = seq(.01, .1, by = .01), 
                 cost = seq(.1, 1, by = .1))
```


```{r}
mysvm = svm(formula = svm_form, 
            data = bbbctrain, 
            gamma =tuned$best.parameters$gamma, 
            cost = tuned$best.parameters$cost)

summary(mysvm)
```

```{r}
svmpredict = predict(mysvm, 
                     bbbctest, 
                     type = "response")

table(pred = svmpredict, true = bbbctest$Choice)
```




>>>>>>> 478c65954d38802cfa506b8174727f63c10a8778


```{r}
caret::confusionMatrix(as.factor(BankTrain$y), as.factor(BankTrain$PredChoice.AIC))
