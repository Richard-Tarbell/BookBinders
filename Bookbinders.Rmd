---
title: "Bookbinders Case Study"
author: "Austin Vanderlyn, Christine Kelly, Richard Tarbell"
date: "2/16/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(logistf)
library(tidyverse)
library(car)
library(broom)
library(DescTools)
library(ROCR)
library(ResourceSelection)
<<<<<<< HEAD
```

```{r, include=FALSE}
BBBC_Train <- read_excel("Data/BBBC-Train.xlsx")
BBBC_Test = read_excel("Data/BBBC-Test.xlsx")
=======
>>>>>>> 67d8d4d30e2a91c7b4155e6c231dabc59edf4cd5
library(e1071)
```


```{r, include=FALSE}
BBBC_Train <- read_excel("Data/BBBC-Train.xlsx",col_names = TRUE)
BBBC_Test <- read_excel("Data/BBBC-Test.xlsx",col_names = TRUE)
```


### I. Executive Summary - Richard
LM

Logit

SVM

### II. The Problem - Austin

### III. Review of Related Literature - Christine

Linear Regression is a parametric model used to find if any explanatory variables influence the response variable.  There are four assumptions for linear regression.  The first is the linear relationship between X and Y.  The second is normal distribution.  The third is homoscedasticity.  Finally, all observations must be independent of each other.  The model finds the best line to predict the behavior of Y, when X is increased by 1 unit.  Y must be continuous for the model to work.

Logistic Regression is used to predict the probability of an outcome based on given variables, or to see how variables are related to the outcome.  For the logistic model Y is binary and not continuous.  Interpretation of the logistic model is not as straightforward and requires an understanding of odds and odds ratio.  

Support Vector Machine (SVM) is a popular model that was developed in the 1990s by Vladimir Vapnik and Corinna Cortes.  This model finds a hyperplane that separates the data as well as possible and allows some misclassification. To accommodate a non-linear boundary between classes, SVM enlarges the feature space using polynomial terms.  The SVM enlarges this feature space, using kernel tricks, in a way that is efficient with these computations. 

All three of these models are beneficial in providing insights and predictions when applied to marketing campaign selections.

### IV. Methodologies

The first model attempted was a linear regression.  This model is not useful for this dataset because the response variable in linear regression must be continuous.  In this case  the response variable Choice is categorical and must be converted to a factor.  If the regression model is used leaving the Choice as numeric, the information it provides is not useful for the criteria we are trying to meet.  

### V. Data / Cleaning - Richard

### VI. Findings 

### VII. Conclusion and Recommendations



### Appendix


##### Data importing and cleaning


Exploration

```{r}
anyNA(BBBC_Train)
anyNA(BBBC_Test)

```


```{r}
str(BBBC_Train)
head(BBBC_Train)
```

Remove observation variable, not relevant
```{r}
BBBC_Train = subset(BBBC_Train, select = -Observation)
BBBC_Test = subset(BBBC_Test, select = -Observation)
```

Change choice and gender to factors for logistic regression model
```{r}
BBBC_Train_Logit = BBBC_Train
BBBC_Test_Logit = BBBC_Test

BBBC_Train_Logit$Choice = as.factor(BBBC_Train_Logit$Choice)
BBBC_Train_Logit$Gender = as.factor(BBBC_Train_Logit$Gender)
BBBC_Test_Logit$Choice = as.factor(BBBC_Test_Logit$Choice)
BBBC_Test_Logit$Gender = as.factor(BBBC_Test_Logit$Gender)
```




### Linear Regression Model


```{r}
#lm.book <- lm(Choice ~ ., data = BBBC_Train_Logit)
#summary(lm.book)
```










### Logistic Regression Model


Create initial logistic regression model
```{r}
glm.train = glm(Choice ~ ., data = BBBC_Train_Logit, family = "binomial")
summary(glm.train)
```

Remove variables with high multicollinearity
```{r}
vif(glm.train)

```

```{r}
glm.train = glm(Choice ~ .-Last_purchase, data = BBBC_Train_Logit, family = "binomial")

vif(glm.train)
```

```{r}
glm.train = glm(Choice ~ .-Last_purchase-First_purchase, data = BBBC_Train_Logit, family = "binomial")

vif(glm.train)
```

Build stepwise model
```{r}
glm.null = glm(Choice ~ 1, data = BBBC_Train_Logit, family = "binomial")
glm.full = glm(Choice ~ .-Last_purchase-First_purchase, data = BBBC_Train_Logit, family = "binomial")

glm.step1 = step(glm.null, scope = list(upper = glm.full), direction = "both", test = "Chisq", trace = FALSE)

summary(glm.step1)
```

Goodness of fit
```{r}
hoslem.test(glm.step1$y, fitted(glm.step1), g = 10)
```

Plot
```{r}
plot(glm.step1, which = 4)
```


```{r}
BBBC_Train_Logit$PredProb = predict.glm(glm.step1, BBBC_Train_Logit, type = "response")
BBBC_Train_Logit$PredChoice = ifelse(BBBC_Train_Logit$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(BBBC_Train_Logit$Choice), as.factor(BBBC_Train_Logit$PredChoice))
```
The accuracy of this model is not the best, but the positive predictive value, which is what we care about, is pretty good.



Run model with test data;
```{r}
BBBC_Test_Logit$PredProb = predict.glm(glm.step1, BBBC_Test_Logit, type = "response")
BBBC_Test_Logit$PredChoice = ifelse(BBBC_Test_Logit$PredProb >= 0.5, 1, 0)
caret::confusionMatrix(as.factor(BBBC_Test_Logit$Choice), as.factor(BBBC_Test_Logit$PredChoice))
```

The model has better accuracy with the test data, and still has a high positive predictive value. It could get better by calibrating the sensitivity and specificity.


Calculate and plot AUC
```{r}
pred_test = predict(glm.step1, BBBC_Test_Logit, type = "response")
response_test = BBBC_Test_Logit$Choice
predict_test = prediction(pred_test, response_test)
auc_test = round(as.numeric(performance(predict_test, measure = "auc")@y.values),3)
perform = performance(predict_test, "tpr","fpr")
plot(perform, colorize = T, main = "ROC Curve")
text(0.5,0.5, paste("AUC:", auc_test))
```



```{r}
plot(unlist(performance(predict_test, "sens")@x.values),
     unlist(performance(predict_test, "sens")@y.values),
     type = "l",
     lwd = 2,
     ylab = "Sensitivity",
     xlab = "Cutoff",
     main = paste("Maximized Cutoff", "AUC: ", auc_test))

par(new = TRUE)

plot(unlist(performance(predict_test, "spec")@x.values),
     unlist(performance(predict_test, "spec")@y.values),
     type = "l",
     lwd = 2,
     col = "red")

axis(4, at=seq(0, 1, 0.2))
mtext("Specificity", side = 4, col = 'red')

min.diff.glm = which.min(abs(unlist(performance(predict_test, "sens")@y.values) - 
                               unlist(performance(predict_test, "spec")@y.values)))
min.x.glm = unlist(performance(predict_test, "sens")@x.values)[min.diff.glm]
min.y.glm = unlist(performance(predict_test, "sens")@y.values)[min.diff.glm]
optimal.glm = min.x.glm

abline(h = min.y.glm, lty = 3)
abline(v = min.x.glm, lty = 3)
text(min.x.glm,0,paste("optimal threshold=",round(optimal.glm,2)), pos = 4)
```
So now we know that the optimal cutoff threshold is 0.23, so we can refit the predictions to optimize the sensitivity and specificity.

Refit predictions and confusion matrix;
```{r}
BBBC_Test_Logit$PredProb = predict.glm(glm.step1, BBBC_Test_Logit, type = "response")
BBBC_Test_Logit$PredChoice = ifelse(BBBC_Test_Logit$PredProb >= 0.23, 1, 0)
caret::confusionMatrix(as.factor(BBBC_Test_Logit$Choice), as.factor(BBBC_Test_Logit$PredChoice))
```

This doesn't actually look that good. The model's accuracy has dropped, as has the specificity and positive predictive value. Based on these numbers, it would most likely be better to go with the model that has a cutoff of 0.5. 

The only thing that might be worth checking out, though, is that in the confusion matrix, it looks like there's a better ratio of customers that actually want to buy versus those where the book is mailed but then returned. 

To calculate profitability of the different models, we use the following information;
cost of mailing = $0.65 / addressee
cost of book = $15
Selling price of book = $31.95
overhead = 0.45*bookcost
Confusion Matrix and Statistics

And for the first stepwise model;
          Reference
Prediction    0    1
         0 1986  110
         1  131   73
```{r}
nummailed = 131
numsold = 73
mailcost = .65*nummailed
bookcost = 15
bookprice = 31.95
overhead = 0.45*bookcost*numsold

profit = (numsold*bookprice) - (mailcost + bookcost*numsold + overhead)
profit
```

For the optimized stepwise model;
          Reference
Prediction    0    1
         0 1490  606
         1   55  149
```{r}
nummailed = 55
numsold = 149
mailcost = .65*numsold
bookcost = 15
bookprice = 31.95
overhead = 0.45*bookcost*numsold

profit2 = (numsold*bookprice) - (mailcost + bookcost*numsold + overhead)
profit2
```
So, even though the optimized model has a lower accuracy rate, it results in greater profitability, so we should go with that.







The best Logit model therefore, is;

log(1/(1-p)) = -0.289 + 1.244*P_Art - 0.088*Frequency - 0.812*Gender1 - 0.294*P_Cook - 0.282*P_DIY + 0.002*Amount_Purchased - 0.196*P_Child, with a cutoff threshold of 0.23 instead of 0.5.

The most influential covariates then, are P_Art(number of art books purchased), Frequency(total number of purchases), Gender, P_Cook(number of cookbooks purchased), P_DIY (number of DIY books purchased), Amount_Purchased (total money spent), and P_Child.

Breaking down how each covariate influences the model;

The odds of a customer buying The Art History of Florence change by a factor of 3.46 with each additional art book purchased, assuming other variables remain constant.

The odds of a customer buying The Art History of Florence change by a factor of 0.915 with each additional book purchased, assuming other variables remain constant.

The odds of a male customer are .443 times that of a female customer, assuming other variables remain constant.

The odds of a customer buying The Art History of Florence change by a factor of 0.745 with each additional cook book purchased, assuming other variables remain constant.

The odds of a customer buying The Art History of Florence change by a factor of 1.002 with each additional dollar spent, assuming other variables remain constant.

The odds of a customer buying The Art History of Florence change by a factor of 1.21 with each additional children's book purchased, assuming other variables remain constant.
```{r}
exp(1.244)
exp(-0.088)
exp(-0.812)
exp(-0.294)
exp(0.002)
exp(0.196)
```

### Midwest Mailing Campaign

Data for the proposed mailing campaign;
50,000 customers
cost of mailing = $0.65 / addressee
cost of book = $15
Selling price of book = $31.95
overhead = 0.45*bookcost

The optimized ratios;
          Reference
Prediction    0    1
         0 1490  606
         1   55  149

The optimized model was based on a model of 2300 customers, so we can scale the ratios to get new totals for mailed and sold;
```{r}
mailedratio = 55/2300
soldratio = 149/2300

nummailed = mailedratio*50000
numsold = soldratio*50000
mailcost = .65*numsold
bookcost = 15
bookprice = 31.95
overhead = 0.45*bookcost*numsold

profit3 = (numsold*bookprice) - (mailcost + bookcost*numsold + overhead)
profit3
```

To calculate expected profit of mailing to the entire list;
```{r}
nummailed = 50000
numsold = soldratio*50000
mailcost = .65*nummailed
bookcost = 15
bookprice = 31.95
overhead = 0.45*bookcost*numsold

profit4 = (numsold*bookprice) - (mailcost + bookcost*numsold + overhead)
profit4
```

It is obviously way more profitable to use the optimized logit model than to mail out the entire list.











### SVM Model

```{r}
BBBC_Train_SVM = BBBC_Train
BBBC_Test_SVM = BBBC_Test
```

```{r}
# Convert to factors
BBBC_Train_SVM$Choice = as.factor(BBBC_Train_SVM$Choice)
BBBC_Train_SVM$Gender = as.factor(BBBC_Train_SVM$Gender)

BBBC_Test_SVM$Choice = as.factor(BBBC_Test_SVM$Choice)
BBBC_Test_SVM$Gender = as.factor(BBBC_Test_SVM$Gender)
```

#### Use training split on BBBC_Train

```{r}
# Splitting data into training and testing sets 70/30 Split
set.seed(1)
tr_ind = sample(nrow(BBBC_Train_SVM), 0.7*nrow(BBBC_Train_SVM), replace=FALSE)
book.train.split = BBBC_Train_SVM[tr_ind,]
book.test.split = BBBC_Train_SVM[-tr_ind,]
```

```{r}
svm_form = Choice ~ .

tuned = tune.svm(svm_form, data = book.train.split, 
                 gamma = seq(.01, .1, by = .01), 
                 cost = seq(.1, 1, by = .1))
```


```{r}
mysvm = svm(formula = svm_form, 
            data = book.train.split, 
            gamma =tuned$best.parameters$gamma, 
            cost = tuned$best.parameters$cost)

summary(mysvm)

```

```{r}
# Predict on the test split
svmpredict = predict(mysvm, 
                     book.test.split, 
                     type = "response")

caret::confusionMatrix(svmpredict, book.test.split$Choice)

```

```{r}
svmpredict = predict(mysvm, 
                     BBBC_Test_SVM, 
                     type = "response")

caret::confusionMatrix(svmpredict, BBBC_Test_SVM$Choice)

```

